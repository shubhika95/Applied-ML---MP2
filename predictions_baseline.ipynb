{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predictions_baseline.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN6QtYlHv2oM4vHfVGQwXsh"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kB89GnGMKmGF","executionInfo":{"status":"ok","timestamp":1607821964103,"user_tz":300,"elapsed":351,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}},"outputId":"a815e561-14bd-4682-897b-b9462e8b1f73"},"source":["import numpy as np\r\n","import scipy.misc # to visualize only\r\n","import os\r\n","from PIL import Image\r\n","import pickle\r\n","import h5py\r\n","import cv2\r\n","import matplotlib.pyplot as plt\r\n","import imutils\r\n","import time\r\n","\r\n","# Mount Google Drive\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","path='/content/drive/My Drive/Colab Notebooks/Comp551_P3/'"],"execution_count":229,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mbnnYwSXJ7fN","executionInfo":{"status":"ok","timestamp":1607821965752,"user_tz":300,"elapsed":462,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}}},"source":["from keras.datasets import mnist\r\n","from keras.models import Sequential\r\n","from keras.layers import Dense\r\n","from keras.layers import Dropout\r\n","from keras.utils import np_utils"],"execution_count":230,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xlm3ZgNKc-l","executionInfo":{"status":"ok","timestamp":1607822010179,"user_tz":300,"elapsed":1150,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}},"outputId":"1f748d61-223c-4359-934a-28482df9b0e5"},"source":["# Load dataset\r\n","f = h5py.File(path+'MNIST_synthetic.h5','r')\r\n","x_train = np.array(f.get('train_dataset'))\r\n","y_train = np.array(f.get('train_labels'))\r\n","x_test = np.array(f.get('test_dataset'))\r\n","\r\n","print(x_train.shape)\r\n","print(y_train.shape)\r\n","print(x_test.shape)"],"execution_count":239,"outputs":[{"output_type":"stream","text":["(56000, 64, 64, 1)\n","(56000, 5)\n","(14000, 64, 64, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TUDuOM99P-Ug","executionInfo":{"status":"ok","timestamp":1607821972241,"user_tz":300,"elapsed":346,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}}},"source":["def isolate_digits (img):\r\n","  x_space_coord = []\r\n","  y_space_coord = []\r\n","\r\n","  sum_thresh = 0    # If sum of all pixels is above this threshold, then we found a digit\r\n","  on_digit = False\r\n","\r\n","  req_num_counts = 3        # We need to detect a digit at least this many number of times to consider it a digit\r\n","  digit_count = 0\r\n","  empty_count = 0\r\n","\r\n","  # Slide a vertical rectangle to detect empty spaces between digits\r\n","  rect_width = 3\r\n","  for x in range(img.shape[1] - rect_width):\r\n","\r\n","    rect_window = img[:,x:x+rect_width]\r\n","    s = np.sum(rect_window)\r\n","\r\n","    # We may have found a digit\r\n","    if s > sum_thresh:\r\n","      digit_count += 1\r\n","      empty_count = 0\r\n","      if  digit_count >= req_num_counts and on_digit == False:\r\n","        on_digit = True\r\n","        x_space_coord.append(x - 5) if len(x_space_coord) % 2 == 0 else x_space_coord.append(x + 5 - rect_width)\r\n","\r\n","    # We may have found a space between digits\r\n","    else:\r\n","      empty_count += 1\r\n","      digit_count = 0\r\n","      if  empty_count >= req_num_counts and on_digit == True:\r\n","        on_digit = False\r\n","        x_space_coord.append(x - 5) if len(x_space_coord) % 2 == 0 else x_space_coord.append(x + 5 - rect_width)\r\n","\r\n","  y1 = 100\r\n","  y2 = 200\r\n","\r\n","  # The segmented images won't have the same shape, we can resize them later\r\n","  digit_imgs = []\r\n","\r\n","  for i, x1 in enumerate(x_space_coord[::2]):\r\n","    x2 = x_space_coord[2 * i + 1]\r\n","    digit_imgs.append(cv2.resize(img[y1:y2, x1:x2], (50, 100)))\r\n","\r\n","  # for i, d_img in enumerate(digit_imgs):\r\n","  #   plt.subplot(1, len(digit_imgs), i+1), plt.imshow(cv2.resize(d_img, (50, 100)))\r\n","  # plt.show()\r\n","\r\n","  return digit_imgs"],"execution_count":232,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzB9SaKtL5vP","executionInfo":{"status":"ok","timestamp":1607821974206,"user_tz":300,"elapsed":355,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}}},"source":["def preprocess_img_data(data, train=True):\r\n","\r\n","  segmented_img_list = []\r\n","  t0 = time.time()\r\n","  count = 0\r\n","  for i, img in enumerate(data):\r\n","\r\n","    # if i == data.shape[0] / 10:\r\n","    # print(i)\r\n","      # print(\"{}% done in {}s\".format(int(i / data.shape[0] * 100), time.time()-t0))\r\n","    # Get image array and resize\r\n","    img = Image.fromarray(np.uint8(img[:,:,0]))\r\n","    newsize = (300, 300) \r\n","    im1 = img.resize(newsize)\r\n","    cimg = cv2.cvtColor(np.array(im1), cv2.COLOR_RGB2BGR)\r\n","    # plt.imshow(cimg)\r\n","    gray = cv2.cvtColor(cimg, cv2.COLOR_BGR2GRAY)\r\n","    # plt.imshow(gray)\r\n","    # Apply Gaussian filter to image\r\n","    blurred = cv2.GaussianBlur(gray, (5,5), 0) \r\n","    # plt.imshow(blurred)\r\n","    # Find edges using the Canny Edge detector\r\n","    edged = cv2.Canny(blurred, 50, 200, 255)\r\n","    # plt.imshow(edged)\r\n","    # Find the digit countours\r\n","    cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\r\n","    cnts = imutils.grab_contours(cnts)\r\n","    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\r\n","    displayCnt = None\r\n","    # print(cnts)\r\n","    # Fill the countours\r\n","    thresh = cv2.threshold(blurred, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\r\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 5))\r\n","    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\r\n","    # plt.imshow(thresh)\r\n","    # Isolate single digits in the image\r\n","    isolated_digits = isolate_digits(thresh)\r\n","\r\n","    # For training we do not need to keep track of which digits are in which image\r\n","    # we only need to know the correspondence between a digit and its label\r\n","    if train:\r\n","      segmented_img_list += isolated_digits\r\n","    else:\r\n","      segmented_img_list.append(isolated_digits)\r\n","\r\n","  return np.array(segmented_img_list)\r\n"],"execution_count":233,"outputs":[]},{"cell_type":"code","metadata":{"id":"lN_ltUywWdYv","executionInfo":{"status":"ok","timestamp":1607821975875,"user_tz":300,"elapsed":335,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}}},"source":["# Remove class=10 from training labels\r\n","def preprocess_labels(labels):\r\n","  labels = labels.flatten()\r\n","  labels = np.delete(labels, np.where(labels == 10))\r\n","  return labels\r\n"],"execution_count":234,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iW498sefTAmH","executionInfo":{"status":"ok","timestamp":1607821977909,"user_tz":300,"elapsed":1430,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}},"outputId":"71f606f3-324d-493b-8866-98754045ced1"},"source":["# Preprocess the images in x_train and x_test\r\n","x_train_segmented = preprocess_img_data(x_train[0:100], True)\r\n","print(\"xtrain: \", x_train_segmented.shape)\r\n","# print(x_train_segmented)\r\n","y_train_segmented = preprocess_labels(y_train[0:100])\r\n","print(\"ytrain: \", y_train_segmented.shape)\r\n","x_test_segmented = preprocess_img_data(x_test[0:100], False)\r\n","print(\"xtest: \", x_test_segmented.shape)\r\n","\r\n","# Flatten images to 1D vector\r\n","num_pixels = x_train_segmented.shape[1] * x_train_segmented.shape[2]\r\n","x_train_segmented = x_train_segmented.reshape((x_train_segmented.shape[0], num_pixels)).astype('float32')\r\n","# x_test_segmented = x_test_segmented.reshape((-1, num_pixels)).astype('float32')\r\n","\r\n","# normalize inputs from 0-255 to 0-1\r\n","x_train_segmented = x_train_segmented / 255\r\n","# x_test_segmented = x_test_segmented / 255\r\n","\r\n","# one hot encode outputs\r\n","y_train_segmented = np_utils.to_categorical(y_train_segmented)\r\n","num_classes = y_train_segmented.shape[1]\r\n","print(\"num_classes_train\", num_classes)\r\n"],"execution_count":235,"outputs":[{"output_type":"stream","text":["xtrain:  (310, 100, 50)\n","ytrain:  (310,)\n","xtest:  (100,)\n","num_classes_train 10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jA9ND1ObJmP7","executionInfo":{"status":"ok","timestamp":1607821979557,"user_tz":300,"elapsed":314,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}}},"source":["# Define baseline model\r\n","def baseline_model():\r\n","\t# create model\r\n","\tmodel = Sequential()\r\n","\tmodel.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\r\n","\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\r\n","\t# Compile model\r\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","\treturn model"],"execution_count":236,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Hf7vupIJ948","executionInfo":{"status":"ok","timestamp":1607821988837,"user_tz":300,"elapsed":8467,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}},"outputId":"fe0d99f5-f51b-45d8-a3f5-e5ae0f257873"},"source":["# Build the model\r\n","model = baseline_model()\r\n","# Fit the model\r\n","# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\r\n","model.fit(x_train_segmented, y_train_segmented, epochs=10, batch_size=200, verbose=2)"],"execution_count":237,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","2/2 - 0s - loss: 10.1963 - accuracy: 0.0806\n","Epoch 2/10\n","2/2 - 0s - loss: 23.5946 - accuracy: 0.3742\n","Epoch 3/10\n","2/2 - 0s - loss: 29.6302 - accuracy: 0.4032\n","Epoch 4/10\n","2/2 - 0s - loss: 23.0100 - accuracy: 0.4452\n","Epoch 5/10\n","2/2 - 0s - loss: 11.4651 - accuracy: 0.6258\n","Epoch 6/10\n","2/2 - 0s - loss: 6.4516 - accuracy: 0.7258\n","Epoch 7/10\n","2/2 - 0s - loss: 4.7788 - accuracy: 0.6613\n","Epoch 8/10\n","2/2 - 0s - loss: 3.6805 - accuracy: 0.6742\n","Epoch 9/10\n","2/2 - 0s - loss: 2.4522 - accuracy: 0.7871\n","Epoch 10/10\n","2/2 - 0s - loss: 2.2691 - accuracy: 0.7871\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fb6bcb25588>"]},"metadata":{"tags":[]},"execution_count":237}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"r4naKhgXjXET","executionInfo":{"status":"ok","timestamp":1607822166177,"user_tz":300,"elapsed":473,"user":{"displayName":"Nicolas B","photoUrl":"","userId":"15884564736597884125"}},"outputId":"b02f0cd7-6d8f-4536-adbe-b8d8cf8223ca"},"source":["# Sample prediction\r\n","index = 10\r\n","y_sample = [10,10,10,10,10]\r\n","\r\n","sample = np.array(x_test_segmented[index])\r\n","print(\"sample_shape: \", sample.shape)\r\n","# print(\"sample\", sample)\r\n","\r\n","# Flatten images to 1D vector\r\n","num_pixels = sample.shape[1] * sample.shape[2]\r\n","sample = sample.reshape((sample.shape[0], num_pixels)).astype('float32')\r\n","sample = sample / 255     # normalize inputs from 0-255 to 0-1\r\n","\r\n","y_pred = model.predict(sample)\r\n","print(\"prediction_probabilities: \", y_pred)\r\n","y_pred = np.argmax(y_pred, axis=1)   # Find label with highest probability\r\n","print(\"highest_label_probabilities: \", y_pred)\r\n","\r\n","num_labels = y_pred.shape[0]\r\n","y_sample[0:num_labels] = y_pred\r\n","print(\"sample_output: \", y_sample)\r\n","\r\n","print(\"Image: \")\r\n","img = Image.fromarray(np.uint8(x_test[index][:,:,0]))\r\n","plt.imshow(img)\r\n"],"execution_count":247,"outputs":[{"output_type":"stream","text":["sample_shape:  (3, 100, 50)\n","prediction_probabilities:  [[5.2315605e-09 3.2268192e-19 7.5764477e-01 4.9433745e-20 2.1094768e-20\n","  7.5948356e-07 2.7852661e-07 3.8034134e-19 2.4235420e-01 2.6684180e-21]\n"," [1.7245330e-15 8.8964876e-17 1.0000000e+00 1.6724433e-16 8.6959836e-22\n","  1.1317423e-19 2.7013074e-16 1.3530643e-23 2.9883662e-11 4.1469320e-27]\n"," [9.9760062e-01 2.9089167e-30 2.3086693e-03 5.3954718e-06 6.1060348e-15\n","  3.6963879e-09 2.5339279e-11 1.6249237e-12 8.5445907e-05 3.2732817e-13]]\n","highest_label_probabilities:  [2 2 0]\n","sample_output:  [2, 2, 0, 10, 10]\n","Image: \n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fb6ccc9fbe0>"]},"metadata":{"tags":[]},"execution_count":247},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}